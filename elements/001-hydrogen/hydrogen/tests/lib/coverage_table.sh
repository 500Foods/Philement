#!/bin/bash

# Script: coverage_table.sh
# Purpose: Analyze gcov files to show detailed line-by-line coverage statistics using tables
# Usage: ./coverage_table.sh

# This script analyzes gcov files generated by Test 11 Unity tests and Test 99 coverage tests
# and displays the results in a formatted table using the tables executable.

# CHANGELOG
# 1.0.4 - 2025-07-14 - Fixed file path extraction using Source: line from gcov files for consistent table alignment
# 1.0.3 - 2025-07-14 - Fixed Tests column to use same gcov processing logic as test 99 for accurate coverage summaries
# 1.0.2 - Added MAGENTA color flag for files with >0 coverage but <50% coverage when >100 instrumented lines
# 1.0.1 - Added YELLOW color flag for files with no coverage in either test type
# 1.0.0 - Initial version

SCRIPT_VER="1.0.4"

# Get script directory and project paths
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/../.." && pwd)"
UNITY_COVS="$PROJECT_ROOT/build/unity/src"
BLACKBOX_COVS="$PROJECT_ROOT/build/coverage/src"
TABLES_EXE="$PROJECT_ROOT/tests/lib/tables"

# Source the coverage common functions for combined analysis
source "$SCRIPT_DIR/coverage-common.sh"

# Load .trial-ignore patterns for consistent filtering
TRIAL_IGNORE_PATTERNS=()
if [[ -f "$PROJECT_ROOT/.trial-ignore" ]]; then
    while IFS= read -r line; do
        # Skip comments and empty lines
        if [[ "$line" =~ ^[[:space:]]*# ]] || [[ -z "$line" ]]; then
            continue
        fi
        # Remove leading ./ if present and add pattern
        pattern="${line#./}"
        TRIAL_IGNORE_PATTERNS+=("$pattern")
    done < "$PROJECT_ROOT/.trial-ignore"
fi

# Function to check if a file should be ignored based on .trial-ignore
should_ignore_file() {
    local filename="$1"
    
    for pattern in "${TRIAL_IGNORE_PATTERNS[@]}"; do
        if [[ "$filename" == *"$pattern"* ]]; then
            return 0  # Should ignore
        fi
    done
    
    return 1  # Should not ignore
}

# Associative arrays to store coverage data from both directories
declare -A unity_covered_lines
declare -A unity_instrumented_lines
declare -A coverage_covered_lines
declare -A coverage_instrumented_lines
declare -A all_files

# Function to analyze a single gcov file and store data
analyze_gcov_file() {
    local gcov_file="$1"
    local coverage_type="$2"  # "unity" or "coverage"
    
    # Input validation - prevent hanging on empty or nonexistent files
    if [[ -z "$gcov_file" ]]; then
        return 1
    fi
    
    if [[ ! -f "$gcov_file" ]]; then
        return 1
    fi
    
    # Use the correct awk parsing logic for gcov format
    local line_counts
    line_counts=$(awk '
        /^[[:space:]]*[0-9]+\*?:[[:space:]]*[0-9]+:/ { covered++; total++ }
        /^[[:space:]]*#####:[[:space:]]*[0-9]+:/ { total++ }
        END {
            if (total == "") total = 0
            if (covered == "") covered = 0
            print total "," covered
        }
    ' "$gcov_file" 2>/dev/null)
    
    local instrumented_lines="${line_counts%,*}"
    local covered_lines="${line_counts#*,}"
    
    # Default to 0 if parsing failed
    instrumented_lines=${instrumented_lines:-0}
    covered_lines=${covered_lines:-0}
    
    # Include all files, even those with 0 instrumented lines, for complete coverage table
    # This ensures the table shows all files that have gcov data, matching test 99's behavior
    
    # Extract relative path from Source: line in gcov file
    local source_line
    source_line=$(grep '^        -:    0:Source:' "$gcov_file" | cut -d':' -f3-)
    local display_path
    if [[ -n "$source_line" ]]; then
        display_path="${source_line#*/hydrogen/}"
    else
        # Fallback to basename
        display_path=$(basename "$gcov_file" .gcov)
        display_path="src/$display_path"
    fi
    
    # Store data in appropriate arrays
    all_files["$display_path"]=1
    if [[ "$coverage_type" == "unity" ]]; then
        unity_covered_lines["$display_path"]=$covered_lines
        unity_instrumented_lines["$display_path"]=$instrumented_lines
    else
        coverage_covered_lines["$display_path"]=$covered_lines
        coverage_instrumented_lines["$display_path"]=$instrumented_lines
    fi
}

# Function to collect and process gcov files from a directory
collect_gcov_files() {
    local build_dir="$1"
    local coverage_type="$2"
    local files_found=0
    
    if [ -d "$build_dir" ]; then
        while IFS= read -r gcov_file; do
            if [[ -f "$gcov_file" ]]; then
                # Use the exact same filtering logic as the working sections
                basename_file=$(basename "$gcov_file")
                if [[ "$basename_file" == "unity.c.gcov" ]] || [[ "$gcov_file" == *"/usr/"* ]]; then
                    continue
                fi
                
                # Skip system include files that show up in Source: lines
                if grep -q "Source:/usr/include/" "$gcov_file" 2>/dev/null; then
                    continue
                fi
                
                if [[ "$basename_file" == "test_"* ]]; then
                    continue
                fi
                
                if [[ "$basename_file" == *"jansson"* ]] || \
                   [[ "$basename_file" == *"json"* ]] || \
                   [[ "$basename_file" == *"curl"* ]] || \
                   [[ "$basename_file" == *"ssl"* ]] || \
                   [[ "$basename_file" == *"crypto"* ]] || \
                   [[ "$basename_file" == *"pthread"* ]] || \
                   [[ "$basename_file" == *"uuid"* ]] || \
                   [[ "$basename_file" == *"zlib"* ]] || \
                   [[ "$basename_file" == *"pcre"* ]]; then
                    continue
                fi
                
                source_file="${basename_file%.gcov}"
                should_ignore=false
                
                if [[ -f "$PROJECT_ROOT/.trial-ignore" ]]; then
                    while IFS= read -r line; do
                        if [[ "$line" =~ ^[[:space:]]*# ]] || [[ -z "$line" ]]; then
                            continue
                        fi
                        pattern="${line#./}"
                        if [[ "$source_file" == *"$pattern"* ]]; then
                            should_ignore=true
                            break
                        fi
                    done < "$PROJECT_ROOT/.trial-ignore"
                fi
                
                if [[ "$should_ignore" == true ]]; then
                    continue
                fi
                
                analyze_gcov_file "$gcov_file" "$coverage_type"
                ((files_found++))
            fi
        done < <(find "$build_dir" -name "*.gcov" -type f 2>/dev/null)
    fi
    
    return "$files_found"
}

# Use the same functions as test 99 for consistency
# Source the coverage functions
source "$SCRIPT_DIR/coverage-unity.sh"
source "$SCRIPT_DIR/coverage-blackbox.sh"

# Use the exact same functions as test 99 to get coverage data
timestamp=$(date '+%Y%m%d_%H%M%S')

# Call the same functions that test 99 uses - this ensures we get the same results
unity_coverage_percentage=$(calculate_unity_coverage "$UNITY_COVS" "$timestamp" 2>/dev/null || echo "0.000")
blackbox_coverage_percentage=$(collect_blackbox_coverage_from_dir "$BLACKBOX_COVS" "$timestamp" 2>/dev/null || echo "0.000")

# The collect_blackbox_coverage_from_dir function already processed all the gcov files
# and stored the results in the detailed files. Let's read that data instead of
# trying to reprocess the files ourselves.

# Read the detailed coverage data that was just generated
unity_files=0
coverage_files=0
if [ -f "${UNITY_COVERAGE_FILE}.detailed" ]; then
    IFS=',' read -r _ _ _ _ unity_instrumented_files unity_covered_files < "${UNITY_COVERAGE_FILE}.detailed"
    unity_files=$unity_covered_files
fi

if [ -f "${BLACKBOX_COVERAGE_FILE}.detailed" ]; then
    IFS=',' read -r _ _ _ _ coverage_instrumented_files coverage_covered_files < "${BLACKBOX_COVERAGE_FILE}.detailed"
    coverage_files=$coverage_covered_files
fi

gcov_files_found=$((unity_files + coverage_files))

if [ "$gcov_files_found" -eq 0 ]; then
    # No gcov files found - create table with zero values instead of exiting
    echo "No gcov files found - generating empty coverage table with zero values"
    
    # Set all totals to zero
    unity_total_covered=0
    unity_total_instrumented=0
    coverage_total_covered=0
    coverage_total_instrumented=0
    combined_total_covered=0
    combined_total_instrumented=0
    
    # Set all percentages to zero
    unity_total_pct="0.000"
    coverage_total_pct="0.000" 
    combined_total_pct="0.000"
    
    # Store the zero combined coverage value for other scripts to use
    echo "$combined_total_pct" > "$COMBINED_COVERAGE_FILE"
    
    # Create temporary directory for JSON files
    temp_dir=$(mktemp -d 2>/dev/null) || { echo "Error: Failed to create temporary directory"; exit 1; }
    layout_json="$temp_dir/coverage_layout.json"
    data_json="$temp_dir/coverage_data.json"
    
    # Create layout JSON for empty table
    cat > "$layout_json" << EOF
{
    "title": "Test Suite Coverage {NC}{RED}———{RESET}{BOLD} Unity: ${unity_total_pct}% {RESET}{RED}———{RESET}{BOLD} Blackbox: ${coverage_total_pct}% {RESET}{RED}———{RESET}{BOLD} Combined: ${combined_total_pct}%",
    "footer": "Generated: $(date) - No coverage data available",
    "footer_position": "right", 
    "theme": "Red",
    "columns": [
        {
            "header": "Status",
            "key": "status",
            "datatype": "text",
            "width": 60
        }
    ]
}
EOF

    # Create data JSON with single message
    cat > "$data_json" << EOF
[
    {
        "status": "No coverage data available - run tests to generate coverage"
    }
]
EOF

    # Use tables executable to render the empty table
    "$TABLES_EXE" "$layout_json" "$data_json" 2>/dev/null || {
        echo "Error: Failed to run tables executable"
        exit 1
    }
    
    # Clean up temporary files
    rm -rf "$temp_dir" 2>/dev/null
    exit 0
fi

# Instead of using our custom gcov processing, use the data from the working functions
# that test 99 uses. These functions already processed all gcov files correctly.

# The collect_blackbox_coverage_from_dir function processes gcov files and stores
# per-file data internally. We need to call the same internal functions it uses
# to get the individual file data.

# Clear our arrays and repopulate them using the working logic
unset unity_covered_lines unity_instrumented_lines coverage_covered_lines coverage_instrumented_lines all_files
declare -A unity_covered_lines
declare -A unity_instrumented_lines
declare -A coverage_covered_lines
declare -A coverage_instrumented_lines
declare -A all_files

# Use the same internal functions that the working coverage functions use
# Source the blackbox coverage functions to get access to internal functions
source "$SCRIPT_DIR/coverage-blackbox.sh"

# Process coverage gcov files using the existing function
collect_gcov_files "$BLACKBOX_COVS" "coverage"

# Process Unity gcov files using the existing function
collect_gcov_files "$UNITY_COVS" "unity"

# Calculate combined coverage for each file
declare -A combined_covered_lines
declare -A combined_instrumented_lines

for file_path in "${!all_files[@]}"; do
    # Use the data we just collected
    u_covered=${unity_covered_lines["$file_path"]:-0}
    u_instrumented=${unity_instrumented_lines["$file_path"]:-0}
    c_covered=${coverage_covered_lines["$file_path"]:-0}
    c_instrumented=${coverage_instrumented_lines["$file_path"]:-0}
    
    # For combined coverage: use max instrumented lines and max covered lines
    # This represents the union of coverage from both test suites
    max_instrumented=$((u_instrumented > c_instrumented ? u_instrumented : c_instrumented))
    max_covered=$((u_covered > c_covered ? u_covered : c_covered))
    
    combined_instrumented_lines["$file_path"]=$max_instrumented
    combined_covered_lines["$file_path"]=$max_covered
done

# Calculate totals for summary first
unity_total_covered=0
unity_total_instrumented=0
coverage_total_covered=0
coverage_total_instrumented=0
combined_total_covered=0
combined_total_instrumented=0

for file_path in "${!all_files[@]}"; do
    # Get Unity data
    u_covered=${unity_covered_lines["$file_path"]:-0}
    u_instrumented=${unity_instrumented_lines["$file_path"]:-0}
    
    # Get Coverage data
    c_covered=${coverage_covered_lines["$file_path"]:-0}
    c_instrumented=${coverage_instrumented_lines["$file_path"]:-0}
    
    # Get Combined data
    combined_covered=${combined_covered_lines["$file_path"]:-0}
    combined_instrumented=${combined_instrumented_lines["$file_path"]:-0}
    
    # Add to totals
    unity_total_covered=$((unity_total_covered + u_covered))
    unity_total_instrumented=$((unity_total_instrumented + u_instrumented))
    coverage_total_covered=$((coverage_total_covered + c_covered))
    coverage_total_instrumented=$((coverage_total_instrumented + c_instrumented))
    combined_total_covered=$((combined_total_covered + combined_covered))
    combined_total_instrumented=$((combined_total_instrumented + combined_instrumented))
done

# Calculate total percentages
unity_total_pct="0.000"
if [ "$unity_total_instrumented" -gt 0 ]; then
    unity_total_pct=$(awk "BEGIN {printf \"%.3f\", ($unity_total_covered / $unity_total_instrumented) * 100}")
fi

coverage_total_pct="0.000"
if [ "$coverage_total_instrumented" -gt 0 ]; then
    coverage_total_pct=$(awk "BEGIN {printf \"%.3f\", ($coverage_total_covered / $coverage_total_instrumented) * 100}")
fi

combined_total_pct="0.000"
if [ "$combined_total_instrumented" -gt 0 ]; then
    combined_total_pct=$(awk "BEGIN {printf \"%.3f\", ($combined_total_covered / $combined_total_instrumented) * 100}")
fi

# Store the combined coverage value for other scripts to use
echo "$combined_total_pct" > "$COMBINED_COVERAGE_FILE"

# Create temporary directory for JSON files
temp_dir=$(mktemp -d 2>/dev/null) || { echo "Error: Failed to create temporary directory"; exit 1; }
layout_json="$temp_dir/coverage_layout.json"
data_json="$temp_dir/coverage_data.json"
    
    # Create layout JSON for the coverage table (now with correct totals)
    cat > "$layout_json" << EOF
{
    "title": "Test Suite Coverage {NC}{RED}———{RESET}{BOLD} Unity: ${unity_total_pct}% {RESET}{RED}———{RESET}{BOLD} Blackbox: ${coverage_total_pct}% {RESET}{RED}———{RESET}{BOLD} Combined: ${combined_total_pct}%",
    "footer": "Generated: $(date)",
    "footer_position": "right",
    "theme": "Red",
    "columns": [
        {
            "header": "Folder",
            "key": "folder",
            "datatype": "text",
            "visible": false,
            "break": true
        },
        {
            "header": "Cover %",
            "key": "combined_coverage_percentage",
            "datatype": "float",
            "justification": "right"
        },
        {
            "header": "Lines",
            "key": "coverage_instrumented",
            "datatype": "int", 
            "justification": "right",
            "summary": "sum"
        },
        {
            "header": "Source File",
            "key": "file_path",
            "datatype": "text",
            "summary": "count",
            "width": 52
        },
        {
            "header": "Unity",
            "key": "unity_covered",
            "datatype": "int",
            "justification": "right",
            "summary": "sum",
            "width": 8
        },
        {
            "header": "Black",
            "key": "coverage_covered",
            "datatype": "int",
            "justification": "right",
            "summary": "sum",
            "width": 8
        },
        {
            "header": "Cover",
            "key": "combined_covered",
            "datatype": "int",
            "justification": "right",
            "summary": "sum",
            "width": 8
        }
    ]
}
EOF

# Start the data JSON array
echo '[' > "$data_json"
first_record=true


# Create array for sorting by folder and file
declare -a file_data=()
for file_path in "${!all_files[@]}"; do
    # Extract folder name from file path using first two levels below src/
    folder="src/"
    if [[ "$file_path" == src/* ]]; then
        # Get the path after src/
        path_after_src="${file_path#src/}"
        if [[ "$path_after_src" == */* ]]; then
            # Has at least one subdirectory
            first_level="${path_after_src%%/*}"
            remaining_path="${path_after_src#*/}"
            
            if [[ "$remaining_path" == */* ]]; then
                # Has at least two subdirectories
                second_level="${remaining_path%%/*}"
                folder="src/${first_level}/${second_level}"
            else
                # Only one subdirectory level
                folder="src/${first_level}"
            fi
        else
            # File directly in src/
            folder="src/"
        fi
    fi
    
    # Store data with sort key: folder:filename
    file_data+=("$folder:$file_path")
done

# Sort by folder, then by filename, ensuring src/hydrogen.c appears first
mapfile -t sorted_file_data < <(printf '%s\n' "${file_data[@]}" | sort -t: -k1,1 -k2,2)

# Generate JSON data for tables
for file_data_entry in "${sorted_file_data[@]}"; do
    folder="${file_data_entry%%:*}"
    file_path="${file_data_entry#*:}"
    # Get Unity data
    u_covered=${unity_covered_lines["$file_path"]:-0}
    u_instrumented=${unity_instrumented_lines["$file_path"]:-0}
    u_percentage="0.000"
    if [ "$u_instrumented" -gt 0 ]; then
        u_percentage=$(awk "BEGIN {printf \"%.3f\", ($u_covered / $u_instrumented) * 100}")
    fi
    
    # Get Coverage data
    c_covered=${coverage_covered_lines["$file_path"]:-0}
    c_instrumented=${coverage_instrumented_lines["$file_path"]:-0}
    c_percentage="0.000"
    if [ "$c_instrumented" -gt 0 ]; then
        c_percentage=$(awk "BEGIN {printf \"%.3f\", ($c_covered / $c_instrumented) * 100}")
    fi
    
    # Get Combined data
    combined_covered=${combined_covered_lines["$file_path"]:-0}
    combined_instrumented=${combined_instrumented_lines["$file_path"]:-0}
    combined_percentage="0.000"
    if [ "$combined_instrumented" -gt 0 ]; then
        combined_percentage=$(awk "BEGIN {printf \"%.3f\", ($combined_covered / $combined_instrumented) * 100}")
    fi
    
    # Calculate maximum coverage percentage
    max_percentage=$(awk "BEGIN {
        u = $u_percentage
        c = $c_percentage
        if (u > c) print u; else print c
    }")
    
    # Highlight files with no coverage in either test type
    display_file_path="$file_path"
    if [[ $u_covered -eq 0 && $c_covered -eq 0 ]]; then
        display_file_path="{YELLOW}$file_path{RESET}"
    elif [[ $combined_covered -gt 0 && $combined_instrumented -gt 100 ]]; then
        # Check if coverage is less than 50%
        coverage_below_50=$(awk "BEGIN {print ($combined_percentage < 50.0) ? 1 : 0}")
        if [[ $coverage_below_50 -eq 1 ]]; then
            display_file_path="{MAGENTA}$file_path{RESET}"
        fi
    fi
    
    # Add comma before record if not first
    if [ "$first_record" = false ]; then
        echo '    ,' >> "$data_json"
    else
        first_record=false
    fi
    
    # Add JSON record with folder field and max coverage
    cat >> "$data_json" << EOF
    {
        "folder": "$folder",
        "max_coverage_percentage": $max_percentage,
        "combined_coverage_percentage": $combined_percentage,
        "file_path": "$display_file_path",
        "unity_covered": $u_covered,
        "unity_instrumented": $u_instrumented,
        "unity_percentage": $u_percentage,
        "coverage_covered": $c_covered,
        "coverage_instrumented": $c_instrumented,
        "coverage_percentage": $c_percentage,
        "combined_covered": $combined_covered,
        "combined_instrumented": $combined_instrumented
    }
EOF
done

# Close the JSON array
echo '' >> "$data_json"
echo ']' >> "$data_json"

# Use tables executable to render the table with robust error handling
if [[ ! -x "$TABLES_EXE" ]]; then
    echo "Warning: tables executable not found at $TABLES_EXE"
    echo "Skipping coverage table generation"
    rm -rf "$temp_dir" 2>/dev/null
    exit 0
fi

# Run with timeout to prevent hanging
if timeout 10s "$TABLES_EXE" "$layout_json" "$data_json" 2>/dev/null; then
    # Success - table generated
    :
else
    exit_code=$?
    echo "Warning: tables executable failed or timed out (exit code: $exit_code)"
    echo "Coverage data available but table generation skipped"
fi

# Clean up temporary files
rm -rf "$temp_dir" 2>/dev/null
